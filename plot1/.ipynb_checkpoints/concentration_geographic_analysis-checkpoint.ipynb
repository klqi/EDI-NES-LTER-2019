{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt user for manual or automated argument\n",
    "while (True):\n",
    "    dataset_type = str(input(\"Please enter 'manual', 'automated', or 'both' for the type of data set you are processing: \"))\n",
    "    # make case insenstive\n",
    "    dataset_type = dataset_type.lower()\n",
    "    if (dataset_type == \"manual\" or dataset_type == \"automated\" or dataset_type == 'both'):\n",
    "        break\n",
    "    print(\"Invalid argument\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask user if running analysis on a new cruise\n",
    "while (True):\n",
    "    reply = str(input('Are you running the analysis on a new cruise? (y/n): ')).lower().strip()\n",
    "    if reply[0] == 'y':\n",
    "        break\n",
    "    if reply[0] == 'n':\n",
    "        break\n",
    "    else:\n",
    "        print(\"Please enter y/n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first call bash script to get necessary file inputs\n",
    "# choose man_query_data.csv as input file during file 1b construction, then names_ids.csv for R prompts\n",
    "# only call if files don't exist or calling on new cruise data so program doesn't need to take full time to run\n",
    "if ( not reply[0] != 'y' or (path.exists('resolved_auto.csv') \n",
    "        and path.exists('resolved_manual.csv') and path.exists('comparison.csv'))):\n",
    "    subprocess.call(['./exec.sh', reply[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read different columns based on dataset type\n",
    "if (dataset_type == 'manual'):\n",
    "    columns = ['permalink', 'namespace_manual', 'worms_higher_order_manual']\n",
    "elif (dataset_type == 'automated'):\n",
    "    columns = ['permalink', 'namespace_automated']\n",
    "else:\n",
    "    columns = ['permalink', 'namespace_manual', 'namespace_automated', 'worms_higher_order_manual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data frame from input\n",
    "samples = pd.read_csv(\"level_1b.csv\", usecols=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get higher order data based on dataset\n",
    "if (dataset_type == 'automated'):\n",
    "    auto_taxon_info = pd.read_csv('resolved_auto.csv', \n",
    "                                  usecols=['name', 'resolved_names', 'resolved_higher_order_fromgnr', 'alt_datasource'])\n",
    "    # merge to get taxa data\n",
    "    samples = pd.merge(samples, auto_taxon_info, how='left', left_on='namespace_automated', right_on='name')\n",
    "    # rename resolved_higher_order column to match\n",
    "    samples.rename(columns={'resolved_higher_order_fromgnr':'worms_higher_order_automated'}, inplace=True)\n",
    "elif (dataset_type == 'manual'):\n",
    "    man_taxon_info = pd.read_csv('resolved_manual.csv', \n",
    "                                  usecols=['name', 'resolved_names', 'alt_datasource'])\n",
    "    # merge to get taxa data\n",
    "    samples = pd.merge(samples, man_taxon_info, how='left', left_on='namespace_manual', right_on='name')\n",
    "else:\n",
    "    auto_taxon_info = pd.read_csv('resolved_auto.csv', \n",
    "                                  usecols=['name', 'resolved_names', 'resolved_higher_order_fromgnr', 'alt_datasource'])\n",
    "    # merge to get taxa data\n",
    "    samples = pd.merge(samples, auto_taxon_info, how='left', left_on='namespace_automated', right_on='name')\n",
    "    # rename resolved_higher_order column to match\n",
    "    samples.rename(columns={'resolved_higher_order_fromgnr':'worms_higher_order_automated'}, inplace=True)\n",
    "    man_taxon_info = pd.read_csv('resolved_manual.csv', \n",
    "                                  usecols=['name', 'resolved_names'])\n",
    "    # merge to get taxa data\n",
    "    samples = pd.merge(samples, man_taxon_info, how='left', left_on='namespace_manual', right_on='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate out roi id from permalink\n",
    "samples['roi'] = samples['permalink']\n",
    "samples.roi = samples.roi.str.slice(68, 74)\n",
    "# gets rid of leading zeros\n",
    "samples.roi = samples.roi.str.lstrip(\"0\")\n",
    "# cut permalink to just be permalink of sample\n",
    "samples.permalink = samples.permalink.str.slice(0, 67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in geolocation data\n",
    "geo_data = pd.read_csv(\"comparison.csv\", usecols=['gps_furuno_latitude', 'pid'])\n",
    "samples = pd.merge(samples, geo_data, how='left', left_on='permalink', right_on='pid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude abiotic particles from other\n",
    "samples = samples[samples['alt_datasource'] != 'OCB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now just use volume from original file- temporary fix\n",
    "volume_data = pd.read_csv('IFCB_count_manual_transect_winter_2018_20190530.csv', \n",
    "                          usecols=['sample_identifier', 'volume_imaged'])\n",
    "volume_data['sample_identifier'] = 'http://ifcb-data.whoi.edu/NESLTER_transect/'+volume_data['sample_identifier'].astype(str)\n",
    "samples = pd.merge(samples, volume_data, how='inner', left_on='permalink', right_on='sample_identifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate abundance based on higher order and latitude\n",
    "if (dataset_type == 'both'):\n",
    "    # need 2 data frames for both\n",
    "    auto_samples = samples.groupby(['gps_furuno_latitude', 'worms_higher_order_automated']).agg(\n",
    "    {\n",
    "        'roi': 'count',\n",
    "        'volume_imaged': 'first'\n",
    "    }\n",
    ").reset_index()\n",
    "    # manual data frame\n",
    "    manual_samples = samples.groupby(['gps_furuno_latitude', 'worms_higher_order_manual']).agg(\n",
    "    {\n",
    "        'roi': 'count',\n",
    "        'volume_imaged': 'first'\n",
    "    }\n",
    ").reset_index()\n",
    "else:\n",
    "    samples = samples.groupby(['gps_furuno_latitude', 'worms_higher_order_{}'.format(dataset_type)]).agg(\n",
    "        {\n",
    "            'roi': 'count',\n",
    "            'volume_imaged': 'first'\n",
    "        }\n",
    "    ).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate concentration from abundance and volume\n",
    "if (dataset_type == 'both'):\n",
    "    # calculate concentration for automated classifications\n",
    "    auto_samples['concentration'] = auto_samples.roi/auto_samples.volume_imaged\n",
    "    # convert to float\n",
    "    auto_samples.concentration = auto_samples.concentration.astype(float)\n",
    "    # calculate concentrations for manual classifications\n",
    "    manual_samples['concentration'] = manual_samples.roi/manual_samples.volume_imaged\n",
    "    # convert to float\n",
    "    manual_samples.concentration = manual_samples.concentration.astype(float)\n",
    "else:\n",
    "    samples['concentration'] = samples.roi/samples.volume_imaged\n",
    "    # convert to float\n",
    "    samples.concentration = samples.concentration.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out to only read Diatoms\n",
    "if (dataset_type == 'both'):\n",
    "    auto_samples = auto_samples[auto_samples['worms_higher_order_automated'] == 'Bacillariophyceae'].reset_index()\n",
    "    manual_samples = manual_samples[manual_samples['worms_higher_order_manual'] == 'Bacillariophyceae'].reset_index()\n",
    "# else:\n",
    "    # samples = samples[samples['worms_higher_order_{}'.format(dataset_type)] == 'Bacillariophyceae'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take duplicate latitudes and add their concentrations together\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "# use unstack()\n",
    "if (dataset_type == 'both'):\n",
    "    grouped = auto_samples.groupby(['gps_furuno_latitude', 'worms_higher_order_automated']).sum()['concentration']\n",
    "    grouped.unstack().plot(ax=ax)\n",
    "    grouped = manual_samples.groupby(['gps_furuno_latitude', 'worms_higher_order_manual'.format(dataset_type)]).sum()['concentration']\n",
    "    grouped.unstack().plot(ax=ax)\n",
    "else:\n",
    "    grouped = samples.groupby(['gps_furuno_latitude', 'worms_higher_order_{}'.format(dataset_type)]).sum()['concentration']\n",
    "    grouped.unstack().plot(ax=ax)\n",
    "# add titles and axes labels\n",
    "plt.xlabel('LTER Stations Latitudes')\n",
    "plt.ylabel('Concentration (#/mL)')\n",
    "plt.title('Taxon Abundance along Transect ({} classifications)'.format(dataset_type))\n",
    "plt.grid(True)\n",
    "# set stations as tick marks\n",
    "ax.set_xticks([41.1967, 41.03, 40.8633, 40.6967, 40.5133, 40.3633, 40.2267, 40.1367, 40.0983, 39.94, 39.7733])\n",
    "# ax.set_xticklabels(np.arange(1,12))\n",
    "# set comments\n",
    "ax.text(41.1967, -15, \"Coast\", size = 15, ha = 'center')\n",
    "ax.text(39.7733, -15, \"Offshore\", size = 15, ha = 'center')\n",
    "# invert x axis\n",
    "ax.invert_xaxis()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
