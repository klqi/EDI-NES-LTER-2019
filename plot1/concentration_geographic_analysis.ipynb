{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import os\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter 'manual', 'automated', or 'both' for the type of data set you are processing: both\n"
     ]
    }
   ],
   "source": [
    "# Prompt user for manual or automated argument\n",
    "while (True):\n",
    "    dataset_type = str(input(\"Please enter 'manual', 'automated', or 'both' for the type of data set you are processing: \"))\n",
    "    # make case insenstive\n",
    "    dataset_type = dataset_type.lower()\n",
    "    if (dataset_type == \"manual\" or dataset_type == \"automated\" or dataset_type == 'both'):\n",
    "        break\n",
    "    print(\"Invalid argument\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Bacillariophyceae\n",
      "2: Dinoflagellata\n",
      "3: Haptophyta\n",
      "4: other than diatoms, dinoflagellates, or haptophytes\n",
      "5: All\n",
      "Please enter a number (1-5) for the type analysis: 1\n"
     ]
    }
   ],
   "source": [
    "# Prompt user for type of analsyis\n",
    "while (True):\n",
    "    options = [\"Bacillariophyceae\", \"Dinoflagellata\", \"Haptophyta\",\n",
    "               \"other than diatoms, dinoflagellates, or haptophytes\", \"All\"]\n",
    "    # Print out options\n",
    "    for i in range(len(options)):\n",
    "        print(str(i+1) + \":\", options[i])\n",
    "    analysis_type = int(input(\"Please enter a number (1-5) for the type analysis: \"))\n",
    "    if (analysis_type in range(1,6)):\n",
    "        analysis_type = str(options[analysis_type-1])\n",
    "        break\n",
    "    print(\"Invalid argument\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you running the analysis on a new cruise? (y/n): y\n"
     ]
    }
   ],
   "source": [
    "# Ask user if running analysis on a new cruise\n",
    "while (True):\n",
    "    reply = str(input('Are you running the analysis on a new cruise? (y/n): ')).lower().strip()\n",
    "    if reply[0] == 'y':\n",
    "        break\n",
    "    if reply[0] == 'n':\n",
    "        break\n",
    "    else:\n",
    "        print(\"Please enter y/n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first call bash script to get necessary file inputs\n",
    "# choose new_man_query_data.csv (60 samples) or man_query_data.csv (72 non-verified) \n",
    "# as input file during file 1b construction, then intermediate_names_ids.csv for R prompts\n",
    "# only call if files don't exist or calling on new cruise data so program doesn't need to take full time to run\n",
    "if ( not reply[0] != 'y' or not (path.exists('resolved_auto.csv') \n",
    "        and path.exists('resolved_manual.csv') and path.exists('geographic_subset.csv'))):\n",
    "    # call bash script\n",
    "    subprocess.call(['./exec.sh', reply[0]])\n",
    "    import ifcb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read different columns based on dataset type\n",
    "if (dataset_type == 'manual'):\n",
    "    columns = ['permalink', 'namespace_manual', 'worms_higher_order_manual', 'identification_manual', 'Biovolume']\n",
    "elif (dataset_type == 'automated'):\n",
    "    columns = ['permalink', 'namespace_automated']\n",
    "else:\n",
    "    columns = ['permalink', 'namespace_manual', 'namespace_automated', \n",
    "               'worms_higher_order_manual', 'identification_manual', 'Biovolume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data frame from input\n",
    "samples = pd.read_csv(\"level_1b.csv\", usecols=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get higher order data based on dataset\n",
    "if (dataset_type == 'automated'):\n",
    "    auto_taxon_info = pd.read_csv('resolved_auto.csv', \n",
    "                                  usecols=['name', 'resolved_names', 'resolved_higher_order_fromgnr', 'alt_datasource'])\n",
    "    # merge to get taxa data\n",
    "    samples = pd.merge(samples, auto_taxon_info, how='left', left_on='namespace_automated', right_on='name')\n",
    "    # rename resolved_higher_order column to match\n",
    "    samples.rename(columns={'resolved_higher_order_fromgnr':'worms_higher_order_automated'}, inplace=True)\n",
    "elif (dataset_type == 'manual'):\n",
    "    man_taxon_info = pd.read_csv('resolved_manual.csv', \n",
    "                                  usecols=['name', 'resolved_names', 'alt_datasource'])\n",
    "    # merge to get taxa data\n",
    "    samples = pd.merge(samples, man_taxon_info, how='left', left_on='namespace_manual', right_on='name')\n",
    "else:\n",
    "    auto_taxon_info = pd.read_csv('resolved_auto.csv', \n",
    "                                  usecols=['name', 'resolved_names', 'resolved_higher_order_fromgnr', 'alt_datasource'])\n",
    "    # merge to get taxa data\n",
    "    samples = pd.merge(samples, auto_taxon_info, how='left', left_on='namespace_automated', right_on='name')\n",
    "    # rename resolved_higher_order column to match\n",
    "    samples.rename(columns={'resolved_higher_order_fromgnr':'worms_higher_order_automated'}, inplace=True)\n",
    "    man_taxon_info = pd.read_csv('resolved_manual.csv', \n",
    "                                  usecols=['name', 'resolved_names'])\n",
    "    # merge to get taxa data\n",
    "    samples = pd.concat([samples, man_taxon_info], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate out roi id from permalink\n",
    "samples['roi'] = samples['permalink']\n",
    "samples.roi = samples.roi.str.slice(68, 74)\n",
    "# gets rid of leading zeros\n",
    "samples.roi = samples.roi.str.lstrip(\"0\")\n",
    "# cut permalink to just be permalink of sample\n",
    "samples.permalink = samples.permalink.str.slice(0, 67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in volume data\n",
    "volumes = pd.read_csv(\"volumes.csv\")\n",
    "# merge with samples\n",
    "samples = pd.merge(samples, volumes, how='left', on='permalink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in geolocation data\n",
    "geo_data = pd.read_csv(\"geographic_subset.csv\", usecols=['gps_furuno_latitude', 'gps_furuno_longitude', \n",
    "                                                         'date', 'pid'])\n",
    "samples = pd.merge(samples, geo_data, how='left', left_on='permalink', right_on='pid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make level_2 summary file from manual data \n",
    "if (dataset_type != 'automated'):\n",
    "    level_2 = samples\n",
    "    level_2 = level_2.groupby(['permalink', 'namespace_manual']).agg(\n",
    "        {\n",
    "            'identification_manual': 'first',\n",
    "            'worms_higher_order_manual': 'first',\n",
    "            'roi': 'count',\n",
    "            'Biovolume': 'sum',\n",
    "            'volume_imaged': 'first',\n",
    "            'date': 'first',\n",
    "            'gps_furuno_latitude': 'first',\n",
    "            'gps_furuno_longitude': 'first'\n",
    "        }\n",
    "    ).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude abiotic particles from other\n",
    "samples = samples[samples['alt_datasource'] != 'OCB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate abundance based on higher order and latitude\n",
    "if (dataset_type == 'both'):\n",
    "    # need 2 data frames for both\n",
    "    auto_samples = samples.groupby(['gps_furuno_latitude', 'worms_higher_order_automated']).agg(\n",
    "    {\n",
    "        'roi': 'count',\n",
    "        'volume_imaged': 'first',\n",
    "    }\n",
    ").reset_index()\n",
    "    # manual data frame\n",
    "    manual_samples = samples.groupby(['gps_furuno_latitude', 'worms_higher_order_manual']).agg(\n",
    "    {\n",
    "        'roi': 'count',\n",
    "        'volume_imaged': 'first'\n",
    "    }\n",
    ").reset_index()\n",
    "else:\n",
    "    samples = samples.groupby(['gps_furuno_latitude', 'worms_higher_order_{}'.format(dataset_type)]).agg(\n",
    "        {\n",
    "            'roi': 'count',\n",
    "            'volume_imaged': 'first'\n",
    "        }\n",
    "    ).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate concentration from abundance and volume\n",
    "if (dataset_type == 'both'):\n",
    "    # calculate concentration for automated classifications\n",
    "    auto_samples['concentration'] = auto_samples.roi/auto_samples.volume_imaged\n",
    "    # convert to float\n",
    "    auto_samples.concentration = auto_samples.concentration.astype(float)\n",
    "    # calculate concentrations for manual classifications\n",
    "    manual_samples['concentration'] = manual_samples.roi/manual_samples.volume_imaged\n",
    "    # convert to float\n",
    "    manual_samples.concentration = manual_samples.concentration.astype(float)\n",
    "else:\n",
    "    samples['concentration'] = samples.roi/samples.volume_imaged\n",
    "    # convert to float\n",
    "    samples.concentration = samples.concentration.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out to only read user specified analysis_type\n",
    "if (dataset_type == 'both'):\n",
    "    if (analysis_type != 'All'):\n",
    "        auto_samples = auto_samples[auto_samples['worms_higher_order_automated'] == analysis_type].reset_index()\n",
    "        manual_samples = manual_samples[manual_samples['worms_higher_order_manual'] == analysis_type].reset_index()\n",
    "else:\n",
    "    if (analysis_type != 'All'):\n",
    "        samples = samples[samples['worms_higher_order_{}'.format(dataset_type)] == analysis_type].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take duplicate latitudes and add their concentrations together\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "# use unstack()\n",
    "if (dataset_type == 'both'):\n",
    "    grouped = auto_samples.groupby(['gps_furuno_latitude', 'worms_higher_order_automated']).sum()['concentration']\n",
    "    grouped.unstack().plot(ax=ax, marker='o')\n",
    "    grouped = manual_samples.groupby(['gps_furuno_latitude', 'worms_higher_order_manual'.format(dataset_type)]).sum()['concentration']\n",
    "    grouped.unstack().plot(ax=ax, marker='o')\n",
    "    L=plt.legend(title=analysis_type)\n",
    "    if (analysis_type != 'All'):\n",
    "        L.get_texts()[0].set_text('automated')\n",
    "        L.get_texts()[1].set_text('manual')\n",
    "else:\n",
    "    grouped = samples.groupby(['gps_furuno_latitude', 'worms_higher_order_{}'.format(dataset_type)]).sum()['concentration']\n",
    "    grouped.unstack().plot(ax=ax, marker='o')\n",
    "# add titles and axes labels\n",
    "plt.xlabel('LTER Stations Latitudes', size = 12)\n",
    "plt.ylabel('Concentration (#/mL)', size = 12)\n",
    "plt.title('Taxon Abundance along Transect ({} classifications)'.format(dataset_type), size = 15)\n",
    "plt.grid(True)\n",
    "# set stations as tick marks\n",
    "ax.set_xticks([41.1967, 41.03, 40.8633, 40.6967, 40.5133, 40.3633, 40.2267, 40.1367, 40.0983, 39.94, 39.7733])\n",
    "# invert x axis\n",
    "ax.invert_xaxis()\n",
    "fig = plt.gcf()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask user if want to save graphs\n",
    "while (True):\n",
    "    reply = str(input('Save output? (y/n): ')).lower().strip()\n",
    "    if reply[0] == 'y':\n",
    "        fig.savefig('taxon_abundance.png', bbox_inches='tight')\n",
    "        level_2.to_csv('level_2.csv', index=None, header=True)\n",
    "        break\n",
    "    if reply[0] == 'n':\n",
    "        break\n",
    "    else:\n",
    "        print(\"Please enter y/n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ifcb-edi]",
   "language": "python",
   "name": "conda-env-ifcb-edi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
